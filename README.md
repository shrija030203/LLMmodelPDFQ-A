# LLMmodelPDFQ-A
This document provides a detailed explanation of the process to fine-tune a BERT model for  question answering (QA) tasks and the subsequent application of this fine-tuned model to  extract answers from PDF documents. 
1. Installing Required Libraries
First, install the necessary libraries using the following commands:
 
2. Importing Necessary Modules
The following imports are essential for the code to function:
 
3. Loading and Preprocessing the SQuAD Dataset â€“ Kaggle data 
The function read_squad is defined to read the SQuAD JSON file and return a dataset structured as dictionaries containing contexts, questions, and answers.
 
4. Splitting the Dataset
The dataset is divided into training and validation sets using the train_test_split function from sklearn. So that we get to train the model on a comparitivly large data scale.
5. Tokenizing the Dataset
Initialized the BERT tokenizer.  
6. Profiling and Preprocessing the Dataset
To profile the preprocessing function and identify potential bottlenecks, use cProfile and pstats
7. Fine-Tuning the Model
After Loading the pre-trained BERT model, Define a function to compute evaluation metrics, specifically the exact match between the predicted and actual start and end positions of answers.
8. Extracting Text from PDF and Answering Questions
Then, we Define a function to extract text from PDF documents using PyMuPDF. Then we Define a function to get answers from the extracted text using the fine-tuned model.
Example usage:
 
Future Aspects
The methodology described above for fine-tuning BERT for question answering tasks can be extended and improved by leveraging more advanced techniques and larger datasets. Here are a few future directions to consider:
Creating a Large Language Model (LLM) from Scratch
1.	Data Collection and Preparation: Gather extensive and diverse datasets to cover a wide range of topics and contexts. This data should be cleaned, preprocessed, and structured appropriately.
2.	Model Architecture: While BERT is highly effective, experimenting with more recent architectures such as customising  transformer models tailored to specific tasks could yield better results.
